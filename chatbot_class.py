# -*- coding: utf-8 -*-
"""chatbot class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v3bG_0wGFFNitKAZjN5ymqZ1FJpIq2om


!pip install transformers

git lfs install
git clone https://huggingface.co/keonju/chat_bot
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
device = "cuda:0" if torch.cuda.is_available() else "cpu"

class chat():
  def __init__(self):
    self.tokenizer = AutoTokenizer.from_pretrained("./chat_bot",padding_side='left')
    self.model = AutoModelForCausalLM.from_pretrained("./chat_bot")

  def __call__(self, input):
    new_user_input_ids = self.tokenizer.encode(input + self.tokenizer.eos_token, return_tensors='pt').to(device)

    chat_history_ids = self.model.generate(
      new_user_input_ids, 
      max_length=1000,
      pad_token_id=self.tokenizer.eos_token_id,
      no_repeat_ngram_size=3,       
      do_sample=True, 
      top_k=100, 
      top_p=0.7,
      temperature = 0.8
    )
    
    return self.tokenizer.decode(chat_history_ids[:, new_user_input_ids.shape[-1]:][0], skip_special_tokens=True)



# chat = chat()
#
# print(chat('나는 오늘 뭐먹지?'))